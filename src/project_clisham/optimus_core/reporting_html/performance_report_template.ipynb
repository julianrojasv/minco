{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expects namespace to be assigned on parameters\n",
    "# set for namespaced execution, otherwise leave empty\n",
    "# namespace = \"\"\n",
    "# point to pipeline parameters\n",
    "conf_params = \"**/data_science/parameters_train_model.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catalog entries\n",
    "conf_test_set_metrics = f\"{namespace}.test_set_metrics\"\n",
    "conf_train_model = f\"{namespace}.train_model\"\n",
    "conf_train_set = f\"{namespace}.train_set\"\n",
    "conf_td = f\"td\"\n",
    "conf_test_set_predictions = f\"{namespace}.test_set_predictions\"\n",
    "conf_feature_importances = f\"{namespace}.train_set_feature_importance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import kedro\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "SMALL_SIZE = 12\n",
    "plt.rc('font', size=SMALL_SIZE)\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)\n",
    "plt.rcParams['figure.figsize'] = [18, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silence warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load kedro context\n",
    "from project_clisham.optimus_core.reporting_html.utils import load_context, mprint\n",
    "from project_clisham.optimus_core.model_helpers import shap as opt_shap\n",
    "\n",
    "logging.getLogger(\"kedro\").setLevel(logging.WARNING)\n",
    "\n",
    "context = load_context()\n",
    "parameters = context.params\n",
    "io = context.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# silence logging\n",
    "logging.getLogger(\"kedro.io\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"kedro.config\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"kedro.pipeline\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"numexpr.utils\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "conf_train_model_params = context.config_loader.get(conf_params)\n",
    "\n",
    "# load data\n",
    "test_prediction_metrics_df = context.catalog.load(conf_test_set_metrics)\n",
    "model = context.catalog.load(conf_train_model)\n",
    "train_dataset = context.catalog.load(conf_train_set)\n",
    "td = context.catalog.load(conf_td)\n",
    "test_predictions_df = context.catalog.load(conf_test_set_predictions)\n",
    "feature_importances = context.catalog.load(conf_feature_importances)\n",
    "feature_importances.name = 'feature_importance'\n",
    "feature_importances.index = [tag + \" - \" + str(td.name(tag)) for tag in feature_importances.index]\n",
    "\n",
    "column_features = parameters[namespace]['model_feature']\n",
    "name_target = parameters[namespace]['model_target']\n",
    "\n",
    "time_col = parameters['timestamp_col_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns from TagDict\n",
    "control_cols = td.select(\"tag_type\", \"control\")\n",
    "feat_cols = td.select(column_features)\n",
    "target_col = td.select('target', name_target)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Perfomance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_timestamp = (\n",
    "    f\"#### generated {datetime.now().strftime('%b-%d-%Y %H:%M:%S %z')} \"\n",
    "    f\"with environment `{context.env}`\"\n",
    ")\n",
    "mprint(report_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_type = (\n",
    "    f\"### Regressor used for Model Run:\\n\"\n",
    "    f\"{conf_train_model_params.get('train_model', {}).get('regressor', {}).get('class', '')}\"\n",
    ")\n",
    "mprint(regressor_type)\n",
    "mprint(\"### Details:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset Performance Metrics for Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction_metrics_df.rename(columns={\"test_perf_metrics\": \" \"}, inplace=True)\n",
    "test_prediction_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 15))\n",
    "feature_importances.sort_values(ascending=True).plot.barh(\n",
    "    ax=ax, title=\"Feature Importance\"\n",
    ")\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_width():.2f}\", (p.get_width() * 1.005, p.get_y() * 1.005))\n",
    "for p in ax.get_yticklabels():\n",
    "    tag = p.get_text().split(\" - \")[0]\n",
    "    if tag in control_cols:\n",
    "        p.set_fontweight(\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "feature_importances.sort_values(ascending=True)[-20:].plot.barh(\n",
    "    ax=ax, title=\"TOP Feature Importance\"\n",
    ")\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_width():.2f}\", (p.get_width() * 1.005, p.get_y() * 1.005))\n",
    "for p in ax.get_yticklabels():\n",
    "    tag = p.get_text().split(\" - \")[0]\n",
    "    if tag in control_cols:\n",
    "        p.set_fontweight(\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset Actual vs. Predicted Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = test_predictions_df[\"prediction\"].rename(\"Predicted\")\n",
    "y_true = test_predictions_df[target_col].rename(\"Actual\")\n",
    "pred_target: pd.DataFrame = pd.concat([y_true, y_preds], 1)\n",
    "scatter_ax = pred_target.plot.scatter(\n",
    "    x=\"Actual\", y=\"Predicted\", figsize=(6, 6), title=\"Actual vs. Predicted\"\n",
    ")\n",
    "xmin_lim, xmax_lim = scatter_ax.get_xlim()\n",
    "scatter_ax.set_ylim(xmin_lim, xmax_lim)\n",
    "plt.plot(\n",
    "    scatter_ax.get_xlim(),\n",
    "    scatter_ax.get_ylim(),\n",
    "    color=\"0.8\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=0.75,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset Actual vs. Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals: pd.Series = (y_true - y_preds).rename(\"Residual\")\n",
    "res_df: pd.DataFrame = pd.concat([y_true, residuals], 1)\n",
    "axis_val = max(abs(residuals.min()), abs(residuals.max())) * 1.05\n",
    "residuals_ax = res_df.plot.scatter(\n",
    "    \"Actual\", \"Residual\", figsize=(10, 3), title=\"Actual vs. Residuals\"\n",
    ")\n",
    "residuals_ax.set_ylim(-1 * axis_val, axis_val)\n",
    "plt.axhline(\n",
    "    y=0, color=\"r\", linestyle=\"-\", linewidth=2,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = train_dataset[[time_col, target_col]].copy()\n",
    "train_pred[time_col] = pd.to_datetime(train_pred[time_col])\n",
    "train_pred[\"prediction\"] = model.predict(train_dataset)\n",
    "train_pred = train_pred.set_index(time_col)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "train_pred.plot(ax=ax)\n",
    "plt.title(\"Train\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_predictions_df[[time_col, target_col, \"prediction\"]].copy()\n",
    "test_pred[time_col] = pd.to_datetime(test_pred[time_col])\n",
    "test_pred = test_pred.set_index(time_col)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "test_pred.plot(ax=ax)\n",
    "plt.title(\"Test\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap Summary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of shap values to calculate\n",
    "n_shap = min(200, len(test_predictions_df))\n",
    "shap_test_df = test_predictions_df.sample(n=n_shap, random_state=0)\n",
    "\n",
    "# number of trees to take into account for SHAP calculation\n",
    "tree_limit = 250\n",
    "\n",
    "selected_model = model.named_steps[\"regressor\"]\n",
    "shap_result = opt_shap.calculate_shap_values(\n",
    "    selected_model,\n",
    "    train_dataset[feat_cols],\n",
    "    shap_test_df[feat_cols],\n",
    "    shap.TreeExplainer,\n",
    "    tree_limit=tree_limit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap_result.shap_values\n",
    "X = shap_result.raw_features\n",
    "feature_names_new = [tag + \" - \" + str(td.name(tag)) for tag in X.columns]\n",
    "shap.summary_plot(shap_values.to_numpy(), X, feature_names=feature_names_new, plot_type=\"dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap results for Feature Columns in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_dependence(col, shap_result):\n",
    "    shap_values = shap_result.shap_values\n",
    "\n",
    "    if isinstance(col, str):\n",
    "        col_int = shap_values.columns.get_loc(col)\n",
    "        if not isinstance(col_int, int):\n",
    "            msg = \"Duplicate column found in shap values? Col was: {}\".format(col)\n",
    "            raise ValueError(msg)\n",
    "    else:\n",
    "        col_int = col\n",
    "\n",
    "    fig = plt.figure()\n",
    "    shap.dependence_plot(\n",
    "        col_int, shap_values.values, shap_result.raw_features, color=\"C0\", show=False\n",
    "    )\n",
    "    plt.xlabel(str(col))\n",
    "    plt.ylabel(\"SHAP value for {}\".format(col))\n",
    "    plt.close(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in feat_cols:\n",
    "    plot = plot_single_dependence(feature, shap_result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimus_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}